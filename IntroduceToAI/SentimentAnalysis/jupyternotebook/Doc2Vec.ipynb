{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import pandas as pd\n",
    "# numpy\n",
    "import numpy as np\n",
    "# random\n",
    "from random import shuffle\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspecting data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegTrainDataPath = \"..\\\\CleanData\\\\train-neg.txt\"\n",
    "PosTrainDataPath = \"..\\\\CleanData\\\\train-pos.txt\"\n",
    "UnLabelTrainDataPath = \"..\\\\CleanData\\\\train-unsup.txt\"\n",
    "testDataPath = \"..\\\\CleanData\\\\test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegLabelTrainDataFrame = pd.read_csv(NegTrainDataPath,header=None)\n",
    "PosLabelTrainDataFrame = pd.read_csv(PosTrainDataPath,header=None)\n",
    "UnLabelTrainDataFrame = pd.read_csv(UnLabelTrainDataPath,header=None)\n",
    "TestDataFrame = pd.read_csv(testDataPath,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegTrainData =  12500\n",
      "PosTrainData =  12500\n",
      "UnLabelTrainData =  50000\n",
      "TestData =  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"NegTrainData = \",NegLabelTrainDataFrame.shape[0])\n",
    "print(\"PosTrainData = \",PosLabelTrainDataFrame.shape[0])\n",
    "print(\"UnLabelTrainData = \",UnLabelTrainDataFrame.shape[0])\n",
    "print(\"TestData = \",TestDataFrame.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: here I omit some very important step, such as how to choose super parameter, you can check ``Doc2Vec.py`` to see my settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, ``mini cout`` is a subtle parameter, I suggest it should be 30 to 50, since each movie occurs 30 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``size`` is another importatne paramter, More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more tech to select `Doc2vec` parameter can be found in here:https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'CleanData\\\\test.txt': 'TEST', 'CleanData\\\\train-neg.txt': 'TRAIN_NEG',\n",
    "            'CleanData\\\\train-pos.txt': 'TRAIN_POS', 'CleanData\\\\train-unsup.txt': 'TRAIN_UNS'}\n",
    "test = Doc2Vec.Train(sources)\n",
    "test.process()\n",
    "test.save('.\\\\Model\\\\imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loadmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('../Persistence/Model/imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  a sample vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5763568e+00,  2.2754261e+00,  1.1008563e+00, -9.7718549e-01,\n",
       "       -2.2184896e-01,  1.5572522e+00,  2.4266410e+00, -1.9701616e+00,\n",
       "       -1.5790352e+00, -1.8281701e+00,  2.2768593e+00,  1.4112015e+00,\n",
       "        1.1728832e+00,  1.1856010e+00, -2.2330038e-02,  1.9760908e+00,\n",
       "       -6.5882629e-01,  3.1324381e-01, -2.6566477e+00, -2.0858505e+00,\n",
       "       -8.9275187e-01,  5.2627224e-01,  4.6037021e-01,  9.3411404e-01,\n",
       "        4.0033208e-03,  1.2601361e+00,  3.7253395e-01, -1.9754703e+00,\n",
       "        8.3788514e-01,  7.2088933e-01,  6.0201448e-01,  8.1771922e-01,\n",
       "        4.6282583e-01,  2.5867789e+00,  1.3548086e+00, -7.5507021e-01,\n",
       "        8.1552511e-01, -1.7249528e+00, -2.1125028e-01, -2.1598797e+00,\n",
       "        1.7278154e+00, -1.0878502e+00, -2.3691947e+00, -3.8591120e-01,\n",
       "       -1.8151284e+00,  1.1379188e+00,  1.0778245e+00, -9.2968976e-01,\n",
       "        2.7547768e-01, -4.6573821e-01, -1.8670373e+00,  1.2685074e+00,\n",
       "        4.6397391e-01, -1.0604509e+00, -1.3412151e+00,  2.0433881e+00,\n",
       "       -2.3682511e+00, -2.6318541e-01, -1.9399327e+00, -4.6237874e-01,\n",
       "        1.3585164e-01,  6.9775015e-01,  1.3348393e+00,  1.0239301e+00,\n",
       "        2.6648492e-02, -8.8612747e-01,  5.0118983e-01, -2.4644050e-01,\n",
       "        6.6196012e-01,  1.0554188e+00, -6.5551603e-01, -2.0817697e+00,\n",
       "        2.0320754e+00, -1.8502271e+00,  1.0798116e+00, -4.5908400e-01,\n",
       "       -5.0936186e-01, -3.1734762e-01, -1.7634342e+00,  6.9658124e-01,\n",
       "        9.5030439e-01, -4.4668806e-01, -1.4122341e+00,  1.7744533e+00,\n",
       "       -1.5900534e+00,  4.8505574e-01, -2.8293324e+00,  1.7376875e+00,\n",
       "       -1.4281958e-01, -8.4488386e-01,  8.2267243e-01,  1.2603040e+00,\n",
       "       -1.0280733e+00,  1.7774420e+00, -1.1131732e+00,  3.6602637e-01,\n",
       "        9.0641046e-01,  2.0249786e+00,  2.1640196e+00,  1.0151681e+00,\n",
       "        6.4865738e-01,  2.5692294e+00,  1.7772077e-01, -1.6196697e+00,\n",
       "        3.6924103e-01,  8.9582115e-01,  1.7440144e+00,  5.3864390e-01,\n",
       "        2.4917743e-01,  2.1334264e+00, -8.7116075e-01,  9.0898877e-01,\n",
       "       -2.2399130e+00, -1.8106004e+00, -3.1533709e-01,  6.8981767e-02,\n",
       "        1.7074610e+00, -2.0401864e+00, -1.9329720e+00,  4.6661618e-01,\n",
       "        3.4861195e-01, -5.8421409e-01, -5.1870298e-01, -1.2500386e-01,\n",
       "       -2.7102378e-01, -4.4568533e-01,  3.1249765e-01,  4.4487655e-01,\n",
       "        9.7172761e-01,  2.8376091e-01,  4.0565252e-01,  3.7300688e-01,\n",
       "       -1.9264824e+00,  4.0284929e-01, -2.2834783e+00, -6.8905687e-01,\n",
       "       -2.3434937e+00, -9.8481059e-01,  1.6244637e+00, -4.5160476e-02,\n",
       "        7.5258277e-02,  1.4596466e+00, -1.5542326e+00,  9.4191152e-01,\n",
       "        1.6725047e+00,  8.6785042e-01,  2.3785135e-01,  4.1084945e-02,\n",
       "       -1.2070012e+00,  3.0575774e+00,  3.1722912e-01,  1.0684354e+00,\n",
       "        3.2285333e-01,  2.7671155e-01, -2.5468197e+00, -1.5908953e+00,\n",
       "        4.3676952e-01, -2.3626752e+00,  1.1472749e+00,  1.2894124e+00,\n",
       "        1.5709052e+00, -1.5450772e+00, -1.4285941e+00,  3.2214576e-01,\n",
       "        1.9004427e+00,  4.0040424e-01, -2.3368178e-01, -1.0974849e+00,\n",
       "        1.3528334e+00, -1.4796576e+00,  4.7796912e-02, -2.6563030e-01,\n",
       "        1.2774284e+00, -5.3146005e-01,  1.4226543e+00,  1.0060003e+00,\n",
       "       -6.3334823e-01, -7.3529488e-01,  1.1329341e+00,  7.5754279e-01,\n",
       "       -1.6328195e+00,  1.2067951e+00,  1.0024452e+00,  5.2227592e-01,\n",
       "        2.1920877e+00,  1.1837109e+00,  1.7834412e+00, -2.6344794e-01,\n",
       "        2.0930102e+00,  9.6107709e-01, -1.7336923e+00,  7.7019250e-01,\n",
       "        1.7939655e-01, -4.0002185e-01, -6.6859663e-02, -6.0160255e-01,\n",
       "        8.9635378e-01, -2.7335092e-01,  3.4252757e-01, -1.9203076e+00,\n",
       "        1.0002913e+00, -9.2160815e-01, -2.9147953e-01,  1.6695839e-01,\n",
       "        3.5967197e-02,  9.3036097e-01, -5.7372636e-01, -2.2269206e+00,\n",
       "       -2.8009582e-01, -3.6588597e-01, -8.0111074e-01,  2.7204645e+00,\n",
       "        4.4758669e-01, -1.4036288e+00, -1.0672472e+00, -2.6900959e-01,\n",
       "        2.6336181e+00, -6.7819238e-01, -2.1480153e+00,  1.5539328e+00,\n",
       "        1.3417982e+00,  7.6324260e-01,  1.4049743e+00, -9.1281521e-01,\n",
       "       -4.2140064e-01, -8.2064062e-01, -4.1562569e-01,  1.3034377e+00,\n",
       "       -8.8864607e-01, -4.3640995e-01,  8.8779950e-01,  1.6927598e+00,\n",
       "       -3.7387222e-01,  1.3817643e+00,  1.9237972e+00,  1.8257781e+00,\n",
       "        7.2756022e-01, -7.6845837e-01,  2.1792452e+00,  1.0420480e-02,\n",
       "        2.6323980e-01,  1.7106835e+00, -9.9279690e-01, -1.8123963e+00,\n",
       "        7.7576917e-01,  9.6012366e-01,  1.2890247e+00,  8.1273723e-01,\n",
       "       -2.8046235e-01,  9.8259398e-04,  6.7117460e-02,  1.4748108e+00,\n",
       "       -7.7605158e-01, -7.6291442e-01,  2.3672900e+00,  5.4685974e-01,\n",
       "        1.2601374e+00, -7.5277364e-01,  2.5552607e-01,  6.4537388e-01,\n",
       "        5.6430548e-01, -9.2654622e-01,  1.2960426e+00,  9.0956491e-01,\n",
       "       -1.1753181e+00,  1.0962149e+00,  1.1152154e-01, -5.8190852e-01,\n",
       "       -9.4445705e-01,  1.7676438e-01, -1.8679713e-01, -1.2752259e-01,\n",
       "       -6.2720716e-01,  2.6736391e+00, -1.9794489e-01,  1.1691806e+00,\n",
       "       -1.0030488e+00, -2.4768350e-01,  4.4839478e-01, -1.0257241e+00,\n",
       "        3.1259784e-01, -2.3204243e+00, -7.1163100e-01, -6.8163699e-01,\n",
       "        8.9776641e-01,  1.5307956e+00, -2.0272338e+00,  1.8230110e-01,\n",
       "        1.4908806e+00, -1.4611380e+00, -5.7277542e-01,  1.8059693e-01,\n",
       "        6.2460268e-01, -2.2588435e-01, -8.3878708e-01,  2.1470146e-01,\n",
       "       -6.3774151e-01, -8.4470046e-01, -1.2794081e+00, -9.6913004e-01,\n",
       "        8.3303291e-01, -9.8054773e-01, -7.3004499e-02, -3.2383901e-01,\n",
       "        5.9918159e-01, -2.8613648e-01, -9.8375297e-01,  1.1870710e+00,\n",
       "        1.9597906e+00,  6.0392731e-01,  2.3301902e+00,  1.6785403e+00,\n",
       "        1.4778475e+00, -1.2246165e+00, -1.1210550e+00, -7.0119452e-01,\n",
       "        2.1159015e+00,  1.0251893e+00,  1.9803758e+00, -1.8590249e-01,\n",
       "       -7.4451528e-03, -4.8523584e-01,  2.8475744e-01,  1.2935252e+00,\n",
       "        1.4264996e+00, -5.6434852e-01,  4.0931821e-01, -4.1532196e-02,\n",
       "        1.2941536e+00, -2.5915637e+00,  1.3599317e+00,  1.3322313e+00,\n",
       "       -4.4025367e-01,  1.1613595e+00, -3.9393127e-01, -1.4949244e+00,\n",
       "        1.2802851e+00,  2.3016188e+00,  3.7007776e-01, -9.3686998e-02,\n",
       "        1.9861478e-01,  7.4688756e-01,  9.5299363e-01, -1.1305468e+00,\n",
       "       -3.4748963e-01, -1.7932453e+00,  1.4171864e+00, -1.4317584e+00,\n",
       "        8.3256549e-01, -1.0619750e+00,  5.2656972e-01, -7.1288101e-02,\n",
       "        2.5666150e-01, -8.3031535e-01, -1.7041837e+00,  1.8757972e+00,\n",
       "        1.0735180e+00,  6.2904710e-01, -2.5243616e-01, -1.5635294e+00,\n",
       "        6.9846034e-01,  4.6826724e-02,  8.9036554e-01,  1.7753276e-01,\n",
       "       -9.9006206e-01, -8.0597186e-01, -2.3143859e+00,  1.4439114e+00,\n",
       "        8.2743466e-01, -5.6022292e-01,  1.0375687e+00,  3.0409068e-01,\n",
       "        1.9882508e-01,  7.5005984e-01, -2.1015206e-01, -8.2257551e-01,\n",
       "       -1.6201903e+00, -2.9954538e+00, -1.1847036e+00,  3.4196377e+00,\n",
       "       -1.4788926e-01,  3.3806849e-01,  4.0660509e-01,  1.1176351e+00,\n",
       "        1.1094689e+00, -8.5219663e-01,  2.5724813e-01,  3.3693423e+00,\n",
       "       -7.8967750e-01, -1.0433103e+00,  2.6104331e-01,  1.0962684e-01,\n",
       "       -4.2970052e-01,  3.9050552e-01,  8.5911028e-02,  8.0661786e-01,\n",
       "       -6.1987138e-01,  4.9188033e-01, -1.8927613e+00,  2.1168284e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check sentiment words, like ``good``,``suck``,find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.5357365608215332),\n",
       " ('decent', 0.52956223487854),\n",
       " ('bad', 0.4647522568702698),\n",
       " ('fine', 0.43398022651672363),\n",
       " ('nice', 0.40893077850341797),\n",
       " ('terrific', 0.390277236700058),\n",
       " ('cool', 0.384939581155777),\n",
       " ('excellent', 0.3649647831916809),\n",
       " ('solid', 0.35547375679016113),\n",
       " ('effective', 0.33740752935409546)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stink', 0.35247352719306946),\n",
       " ('sucked', 0.32474765181541443),\n",
       " ('terrible', 0.32183218002319336),\n",
       " ('horrible', 0.2982178330421448),\n",
       " ('stunk', 0.2886814773082733),\n",
       " ('bad', 0.2821727693080902),\n",
       " ('awful', 0.28032806515693665),\n",
       " ('subpar', 0.24847999215126038),\n",
       " ('hate', 0.24726806581020355),\n",
       " ('evolve', 0.24718019366264343)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('suck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seem pretty good, since our analysis depends on the sentiment words, so their relation is rather important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check model robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8233434557914734),\n",
       " ('men', 0.6354216933250427),\n",
       " ('lady', 0.6248824596405029),\n",
       " ('child', 0.5895017385482788),\n",
       " ('kid', 0.5808984637260437),\n",
       " ('gal', 0.571243941783905),\n",
       " ('teenager', 0.560899555683136),\n",
       " ('daughter', 0.5439508557319641),\n",
       " ('sister', 0.5325612425804138),\n",
       " ('lad', 0.5323701500892639)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','boy'],negative=['man'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run `tensorboard --logdir=..\\Model\\visual` to run visualize result on tensorboard\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def visualize(model, output_path):\n",
    "    meta_file = \"w2x_metadata.tsv\"\n",
    "    placeholder = np.zeros((len(model.wv.index2word), model.wv.vector_size))\n",
    "\n",
    "    with open(os.path.join(output_path,meta_file), 'wb') as file_metadata:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            placeholder[i] = model[word]\n",
    "            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094\n",
    "            if word == '':\n",
    "                print(\"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard\")\n",
    "                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n",
    "            else:\n",
    "                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n",
    "\n",
    "    # define the model without training\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    embedding = tf.Variable(placeholder, trainable = False, name = 'w2x_metadata')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(output_path, sess.graph)\n",
    "\n",
    "    # adding into projector\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "    embed.tensor_name = 'w2x_metadata'\n",
    "    embed.metadata_path = meta_file\n",
    "\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    saver.save(sess, os.path.join(output_path,'w2x_metadata.ckpt'))\n",
    "    print('Run `tensorboard --logdir={0}` to run visualize result on tensorboard'.format(output_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Word2Vec.load(\"..\\\\Model\\\\imdb.d2v\")\n",
    "    visualize(model,\"..\\\\Model\\\\visual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `tensorboard --logdir=..\\Model\\visual` to run visualize result on tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![52776363609](http://wx4.sinaimg.cn/mw690/0060lm7Tly1fs1iq0ghwzj31gq0pt12q.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review \"suck\" using euclidean  distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![52776398234](http://wx1.sinaimg.cn/mw690/0060lm7Tly1fs1iozlqgej30v10m4mym.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save vector for LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for LSTM use, we need to convert document to a list of vector, I choose to vectorize firt `500` words as their feature, so every document has `500` feature, each feature is a `400` length vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDataForRNN(self, feature=500):\n",
    "        \"\"\"\n",
    "        save X_train,Y_train,X_test to ``./Persistence/``\\n\n",
    "        X_train.shape = (25000,feature_size,model.vector.size)\\n\n",
    "        Y_train.shape = (25000,2)  [1,0] for ``positive`` [0,1] for ``negative``\\n\n",
    "        too big to return, read data from ``./Persistence``\n",
    "        \"\"\"\n",
    "        sources = {'CleanData\\\\test.txt': 'TEST', 'CleanData\\\\train-neg.txt': 'TRAIN_NEG',\n",
    "                   'CleanData\\\\train-pos.txt': 'TRAIN_POS', 'CleanData\\\\train-unsup.txt': 'TRAIN_UNS'}\n",
    "\n",
    "        # dict sentences_dict[\"TRAIN_NEG_0\"] = [\"a\",\"b\",...\"last word\"]\n",
    "        sentences = LabeledLineSentence(sources)\n",
    "        sentences_array = sentences.to_array()\n",
    "        sentences_dict = {}\n",
    "        for i in range(0, len(sentences_array)):\n",
    "            sentences_dict[sentences_array[i][1][0]] = sentences_array[i][0]\n",
    "\n",
    "        # Index2word is a list that contains the names of the words in\n",
    "        # the model's vocabulary. Convert it to a set, for speed\n",
    "        index2word_set = set(self.model.wv.index2word)\n",
    "\n",
    "        # when short of word, using zero instead\n",
    "        empty_word = np.zeros(self.model.vector_size,dtype='float16')\n",
    "\n",
    "\n",
    "        X_train = np.zeros((25000, feature, self.model.vector_size),dtype='float16')\n",
    "        Y_train = np.zeros(25000)\n",
    "\n",
    "\n",
    "\n",
    "        # get first 1000 feature vectors from 1000 words\n",
    "        for i in range(12500):\n",
    "            prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "            prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "            # length of document\n",
    "            len1 = len(sentences_dict[prefix_train_pos])\n",
    "            len2 = len(sentences_dict[prefix_train_neg])\n",
    "            cout = j = 0\n",
    "            # for pos document\n",
    "            while cout < feature:\n",
    "                if j < len1:\n",
    "                    word = sentences_dict[prefix_train_pos][j]\n",
    "                    if word in index2word_set:\n",
    "                        X_train[i, cout, :] = self.model[word]\n",
    "                        cout += 1\n",
    "                else:\n",
    "                    X_train[i, cout, :] = empty_word\n",
    "                    cout += 1\n",
    "                j += 1\n",
    "\n",
    "            # reset j/cout, for neg document\n",
    "            cout = j = 0\n",
    "            while cout < feature:\n",
    "                if j < len2:\n",
    "                    word = sentences_dict[prefix_train_neg][j]\n",
    "                    if word in index2word_set:\n",
    "                        X_train[12500+i, cout, :] = self.model[word]\n",
    "                        cout += 1\n",
    "                else:\n",
    "                    X_train[12500+i, cout, :] = empty_word\n",
    "                    cout += 1\n",
    "                j += 1\n",
    "\n",
    "            Y_train[i] = 1\n",
    "            Y_train[12500 + i] = 0\n",
    "\n",
    "        with open('./Persistence/LSTM/X_train.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        with open('./Persistence/LSTM/Y_train.pickle', 'wb') as f:\n",
    "            pickle.dump(Y_train, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        X_test =np.zeros((25000, feature, self.model.vector_size),dtype='float16')\n",
    "\n",
    "        for i in range(25000):\n",
    "            prefix_test = 'TEST_' + str(i)\n",
    "            len1 = len(sentences_dict[prefix_test])\n",
    "            cout = j = 0\n",
    "            while cout < feature:\n",
    "                if j < len1:\n",
    "                    word = sentences_dict[prefix_test][j]\n",
    "                    if word in index2word_set:\n",
    "                        X_test[i, cout, :] = self.model[word]\n",
    "                        cout += 1\n",
    "                else:\n",
    "                    X_test[i, cout, :] = empty_word\n",
    "                    cout += 1\n",
    "                j += 1\n",
    "\n",
    "        with open('./Persistence/LSTM/X_test.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        print(\"save narray success to ./Persistence\")\n",
    "\n",
    "        # return X_train,Y_train,X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
