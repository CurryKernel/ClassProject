{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scott\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import pandas as pd\n",
    "# numpy\n",
    "import numpy as np\n",
    "# random\n",
    "from random import shuffle\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspecting data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegTrainDataPath = \"..\\\\CleanData\\\\train-neg.txt\"\n",
    "PosTrainDataPath = \"..\\\\CleanData\\\\train-pos.txt\"\n",
    "UnLabelTrainDataPath = \"..\\\\CleanData\\\\train-unsup.txt\"\n",
    "testDataPath = \"..\\\\CleanData\\\\test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegLabelTrainDataFrame = pd.read_csv(NegTrainDataPath,header=None)\n",
    "PosLabelTrainDataFrame = pd.read_csv(PosTrainDataPath,header=None)\n",
    "UnLabelTrainDataFrame = pd.read_csv(UnLabelTrainDataPath,header=None)\n",
    "TestDataFrame = pd.read_csv(testDataPath,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegTrainData =  12500\n",
      "PosTrainData =  12500\n",
      "UnLabelTrainData =  50000\n",
      "TestData =  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"NegTrainData = \",NegLabelTrainDataFrame.shape[0])\n",
    "print(\"PosTrainData = \",PosLabelTrainDataFrame.shape[0])\n",
    "print(\"UnLabelTrainData = \",UnLabelTrainDataFrame.shape[0])\n",
    "print(\"TestData = \",TestDataFrame.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'CleanData\\\\test.txt': 'TEST', 'CleanData\\\\train-neg.txt': 'TRAIN_NEG',\n",
    "            'CleanData\\\\train-pos.txt': 'TRAIN_POS', 'CleanData\\\\train-unsup.txt': 'TRAIN_UNS'}\n",
    "test = Doc2Vec.Train(sources)\n",
    "test.process()\n",
    "test.save('.\\\\Model\\\\imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loadmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('../Model/imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  a sample vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.62691534e-01,  -7.76247692e+00,   2.40531802e+00,\n",
       "        -3.79832411e+00,   4.68630075e+00,   6.01266742e-01,\n",
       "         2.65584946e+00,   2.48005986e-03,  -3.24314213e+00,\n",
       "        -8.89416933e-02,   2.93580627e+00,  -7.79288113e-01,\n",
       "         1.55539155e+00,  -3.08664632e+00,   6.55950308e+00,\n",
       "         2.63487029e+00,   3.27819824e+00,  -7.67582417e-01,\n",
       "         6.47629261e-01,  -1.07775927e+00,   3.05098981e-01,\n",
       "        -2.86874801e-01,  -3.14541960e+00,   9.44287360e-01,\n",
       "         1.80172253e+00,  -3.12100601e+00,  -2.05273628e+00,\n",
       "        -5.92697084e-01,  -4.72062302e+00,  -6.03741312e+00,\n",
       "        -3.10472775e+00,   2.27795315e+00,  -5.60661602e+00,\n",
       "         3.58796453e+00,  -6.22448087e-01,   1.44651663e+00,\n",
       "        -8.29010382e-02,  -1.95712864e+00,  -5.06302655e-01,\n",
       "         1.62629855e+00,   2.58372545e+00,   1.21157682e+00,\n",
       "        -1.70597863e+00,   1.65418839e+00,   1.13596499e+00,\n",
       "         2.81472373e+00,  -5.64618492e+00,  -4.42828560e+00,\n",
       "        -3.87430906e+00,   2.51064110e+00,  -3.14564562e+00,\n",
       "         7.43221879e-01,  -1.86803317e+00,  -5.72782576e-01,\n",
       "         6.71011806e-01,  -2.92667770e+00,   2.25599074e+00,\n",
       "        -1.96786284e+00,  -9.98493969e-01,   9.75389671e+00,\n",
       "        -1.13252175e+00,  -4.14925528e+00,  -5.21203995e+00,\n",
       "        -6.76068068e+00,  -3.90878749e+00,   1.96954262e+00,\n",
       "         2.74700618e+00,  -3.47910023e+00,  -5.93412209e+00,\n",
       "        -6.38014674e-01,   4.84356284e-01,  -4.96565294e+00,\n",
       "        -2.18422461e+00,  -4.44803476e+00,  -4.34039879e+00,\n",
       "         1.57309639e+00,   4.26661825e+00,   2.37578487e+00,\n",
       "         2.41723037e+00,  -1.87417138e+00,  -2.60665584e+00,\n",
       "         2.42834735e+00,   3.64462280e+00,  -5.50337934e+00,\n",
       "        -2.29109383e+00,  -1.84121192e+00,   9.77308273e-01,\n",
       "         3.69752216e+00,  -8.52388963e-02,  -3.94283628e+00,\n",
       "        -1.08657146e+00,   3.58684802e+00,  -3.65730309e+00,\n",
       "        -5.71785688e+00,   4.11086178e+00,   1.71784294e+00,\n",
       "         9.39648807e-01,   7.53803253e-01,  -1.45982194e+00,\n",
       "        -6.76372588e-01], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7674194574356079),\n",
       " ('great', 0.7440007328987122),\n",
       " ('fine', 0.7338722944259644),\n",
       " ('bad', 0.7250358462333679),\n",
       " ('solid', 0.6936144232749939),\n",
       " ('nice', 0.6750378012657166),\n",
       " ('well', 0.6653556823730469),\n",
       " ('fantastic', 0.6441545486450195),\n",
       " ('terrible', 0.6301878690719604),\n",
       " ('excellent', 0.6274154186248779)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8233434557914734),\n",
       " ('men', 0.6354216933250427),\n",
       " ('lady', 0.6248824596405029),\n",
       " ('child', 0.5895017385482788),\n",
       " ('kid', 0.5808984637260437),\n",
       " ('gal', 0.571243941783905),\n",
       " ('teenager', 0.560899555683136),\n",
       " ('daughter', 0.5439508557319641),\n",
       " ('sister', 0.5325612425804138),\n",
       " ('lad', 0.5323701500892639)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','boy'],negative=['man'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.95239973, -1.24969399,  3.29956985, -3.90462565,  4.61045265,\n",
       "        5.51968288,  0.39832857,  4.1443119 , -1.29792786, -0.34585753,\n",
       "       -4.00481272,  0.86193132,  3.09168172,  3.9264338 , -1.76124251,\n",
       "       -5.4568038 ,  0.59171295,  3.06078196, -4.07117271, -0.18574995,\n",
       "        3.2747879 , -0.46941936,  0.03911133, -4.64472246,  4.89604139,\n",
       "       -3.14762974, -1.24758673, -0.30737233, -0.23196392,  0.84912157,\n",
       "       -4.2230196 , -0.7549361 ,  4.38383293,  2.61794567, -0.55559188,\n",
       "       -1.1644603 ,  1.79213178, -1.66251159, -0.5805034 ,  1.87585771,\n",
       "       -3.2442956 ,  2.58748746, -6.56832647, -2.94067574,  1.32970643,\n",
       "        2.68853927, -1.69721258, -1.79148626, -8.86662197,  1.20294356,\n",
       "       -1.77326357, -0.10407368,  2.62547898, -0.50494879, -2.01088643,\n",
       "        0.09080964,  2.30544114, -2.91932392, -4.02691031,  8.32335854,\n",
       "        0.53457642,  3.04086828,  4.17223024, -0.7443428 ,  2.18706608,\n",
       "       -2.46791577,  0.38441673,  8.10077477,  1.06671786, -1.57949018,\n",
       "        1.61706913, -1.54608476,  0.95284015, -2.78473353, -2.80193591,\n",
       "       -4.89531946, -2.20222664, -0.73161048,  1.46036208,  0.5364278 ,\n",
       "        1.39427269, -0.15211868, -1.13897371,  2.20785069, -5.18967009,\n",
       "        0.81071365,  0.75897986,  6.12849569,  2.80067253,  5.58790922,\n",
       "        1.29690516,  1.4529078 , -0.57433176,  1.20558894,  2.21884632,\n",
       "       -1.19158292, -0.63834053, -4.81173563, -4.01016235, -1.23843253], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"TEST_24810\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run `tensorboard --logdir=..\\Model\\visual` to run visualize result on tensorboard\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def visualize(model, output_path):\n",
    "    meta_file = \"w2x_metadata.tsv\"\n",
    "    placeholder = np.zeros((len(model.wv.index2word), model.wv.vector_size))\n",
    "\n",
    "    with open(os.path.join(output_path,meta_file), 'wb') as file_metadata:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            placeholder[i] = model[word]\n",
    "            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094\n",
    "            if word == '':\n",
    "                print(\"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard\")\n",
    "                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n",
    "            else:\n",
    "                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n",
    "\n",
    "    # define the model without training\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    embedding = tf.Variable(placeholder, trainable = False, name = 'w2x_metadata')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(output_path, sess.graph)\n",
    "\n",
    "    # adding into projector\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "    embed.tensor_name = 'w2x_metadata'\n",
    "    embed.metadata_path = meta_file\n",
    "\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    saver.save(sess, os.path.join(output_path,'w2x_metadata.ckpt'))\n",
    "    print('Run `tensorboard --logdir={0}` to run visualize result on tensorboard'.format(output_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Word2Vec.load(\"..\\\\Model\\\\imdb.d2v\")\n",
    "    visualize(model,\"..\\\\Model\\\\visual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `tensorboard --logdir=..\\Model\\visual` to run visualize result on tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![52776363609](http://wx3.sinaimg.cn/mw690/0060lm7Tly1frupr6lqnej31gm0ptjwg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review \"suck\" using euclidean  distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![52776398234](http://wx1.sinaimg.cn/mw690/0060lm7Tly1frupx4my6hj30ox0m5mzj.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save vector for LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature = 1000, dimension =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDataForRNN(self, feature=1000):\n",
    "        \"\"\"\n",
    "        save X_train,Y_train,X_test to ``./Persistence/``\\n\n",
    "        X_train.shape = (25000,feature_size,model.vector.size)\\n\n",
    "        Y_train.shape = (25000,2)  [1,0] for ``positive`` [0,1] for ``negative``\\n\n",
    "        too big to return, read data from ``./Persistence``\n",
    "        \"\"\"\n",
    "        sources = {'CleanData\\\\test.txt': 'TEST', 'CleanData\\\\train-neg.txt': 'TRAIN_NEG',\n",
    "                   'CleanData\\\\train-pos.txt': 'TRAIN_POS', 'CleanData\\\\train-unsup.txt': 'TRAIN_UNS'}\n",
    "\n",
    "        # dict sentences_dict[\"TRAIN_NEG_0\"] = [\"a\",\"b\",...\"last word\"]\n",
    "        sentences = LabeledLineSentence(sources)\n",
    "        sentences_array = sentences.to_array()\n",
    "        sentences_dict = {}\n",
    "        for i in range(0, len(sentences_array)):\n",
    "            sentences_dict[sentences_array[i][1][0]] = sentences_array[i][0]\n",
    "\n",
    "        # data for RNN get first 1000 feature vectors from 1000 words\n",
    "        X_train = np.zeros(\n",
    "            shape=(25000, feature, self.model.vector_size)).astype('float16')\n",
    "        # Y_train = np.zeros(shape=(25000, 2)).astype('float16')\n",
    "        Y_train = np.zeros(25000)\n",
    "\n",
    "        # one-hot for Y_train [1][0] for positive [0][1] for negative\n",
    "        # pos = np.zeros(shape=(1, 2)).astype('float16')\n",
    "        # pos[0][0] = 1\n",
    "        # neg = np.zeros(shape=(1, 2)).astype('float16')\n",
    "        # neg[0][1] = 1\n",
    "\n",
    "        # when short of word, using zero instead\n",
    "        empty_word = np.zeros(self.model.vector_size).astype('float16')\n",
    "\n",
    "        # get first 1000 feature vectors from 1000 words\n",
    "        for i in range(12500):\n",
    "            prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "            prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "            len1 = len(sentences_dict[prefix_train_pos])\n",
    "            len2 = len(sentences_dict[prefix_train_neg])\n",
    "            for j in range(feature):\n",
    "                if j < len1:\n",
    "                    # have enough word\n",
    "                    X_train[i, j, :] = self.model[sentences_dict[prefix_train_pos][j]]\n",
    "                else:\n",
    "                    X_train[i, j, :] = empty_word\n",
    "\n",
    "                if j < len2:\n",
    "                    X_train[12500+i, j,\n",
    "                            :] = self.model[sentences_dict[prefix_train_neg][j]]\n",
    "                else:\n",
    "                    X_train[12500+i, j, :] = empty_word\n",
    "\n",
    "            Y_train[i] = 1\n",
    "            Y_train[12500 + i] = 0\n",
    "            # Y_train[i, :] = pos\n",
    "            # Y_train[12500 + i, :] = neg\n",
    "\n",
    "        with open('./Persistence/X_train.pickle', 'wb') as f:\n",
    "            pickle.dump(X_train, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        with open('./Persistence/Y_train.pickle', 'wb') as f:\n",
    "            pickle.dump(Y_train, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        X_test = np.zeros(\n",
    "            shape=(25000, feature, self.model.vector_size)).astype('float16')\n",
    "\n",
    "        for i in range(25000):\n",
    "            prefix_test = 'TEST_' + str(i)\n",
    "            len1 = len(sentences_dict[prefix_test])\n",
    "            for j in range(feature):\n",
    "                if j < len1:\n",
    "                    X_test[i, j, :] = self.model[sentences_dict[prefix_test][j]]\n",
    "                else:\n",
    "                    X_test[i, j, :] = empty_word\n",
    "\n",
    "        with open('./Persistence/X_test.pickle', 'wb') as f:\n",
    "            pickle.dump(X_test, f, protocol=4)\n",
    "            f.close()\n",
    "\n",
    "        print(\"save narray success to ./Persistence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
